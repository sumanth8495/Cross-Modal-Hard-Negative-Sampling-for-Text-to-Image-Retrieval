{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "72a8b6e2789a4d86a7d46b213c87b9e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b996976df8024b6ea95f5dba1dd101a8",
              "IPY_MODEL_4060bc0609c94c53a24a0d6654f1862d",
              "IPY_MODEL_b8ad8a3a27fd4dbe96790c920c3339fc"
            ],
            "layout": "IPY_MODEL_86c1262f5b844c13971320054d02f488"
          }
        },
        "b996976df8024b6ea95f5dba1dd101a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a1458a9a2ab452bac32443f0491b34d",
            "placeholder": "​",
            "style": "IPY_MODEL_6363c8f072e746b2ad2f3f13192c6725",
            "value": "open_clip_model.safetensors: 100%"
          }
        },
        "4060bc0609c94c53a24a0d6654f1862d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de20ade2bc764f1b8c2d7203941c4ac3",
            "max": 605143284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_598434db3b1b461ea9706956fe35faa8",
            "value": 605143284
          }
        },
        "b8ad8a3a27fd4dbe96790c920c3339fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baa031b9b54e4d50ad015445680f9b03",
            "placeholder": "​",
            "style": "IPY_MODEL_4e593131c2bb4087bbe1bdf869d0810c",
            "value": " 605M/605M [00:03&lt;00:00, 288MB/s]"
          }
        },
        "86c1262f5b844c13971320054d02f488": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a1458a9a2ab452bac32443f0491b34d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6363c8f072e746b2ad2f3f13192c6725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de20ade2bc764f1b8c2d7203941c4ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "598434db3b1b461ea9706956fe35faa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "baa031b9b54e4d50ad015445680f9b03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e593131c2bb4087bbe1bdf869d0810c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TopKWdStqkhf",
        "outputId": "8c8b4504-ee96-4c1e-943d-87a871e72238",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting open_clip_torch\n",
            "  Downloading open_clip_torch-3.2.0-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2024.11.6)\n",
            "Collecting ftfy (from open_clip_torch)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.6.2)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.14)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.10.5)\n",
            "Downloading open_clip_torch-3.2.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ftfy, open_clip_torch\n",
            "Successfully installed ftfy-6.3.1 open_clip_torch-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install open_clip_torch torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"\\nGPU Name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFf5c4Qaqo6h",
        "outputId": "78765d6b-ead0-4476-a85d-33577b4d7174",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using device: cuda\n",
            "\n",
            "GPU Available: True\n",
            "\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import open_clip\n",
        "\n",
        "print(\"\\nAvailable CLIP models:\")\n",
        "print(open_clip.list_models())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMEfPkaJqyke",
        "outputId": "e3765de9-5d30-4a2b-cdc5-cc1b28ba6a3d",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Available CLIP models:\n",
            "['coca_base', 'coca_roberta-ViT-B-32', 'coca_ViT-B-32', 'coca_ViT-L-14', 'convnext_base', 'convnext_base_w', 'convnext_base_w_320', 'convnext_large', 'convnext_large_d', 'convnext_large_d_320', 'convnext_small', 'convnext_tiny', 'convnext_xlarge', 'convnext_xxlarge', 'convnext_xxlarge_320', 'EVA01-g-14', 'EVA01-g-14-plus', 'EVA02-B-16', 'EVA02-E-14', 'EVA02-E-14-plus', 'EVA02-L-14', 'EVA02-L-14-336', 'MobileCLIP2-B', 'MobileCLIP2-L-14', 'MobileCLIP2-S0', 'MobileCLIP2-S2', 'MobileCLIP2-S3', 'MobileCLIP2-S4', 'MobileCLIP-B', 'MobileCLIP-S1', 'MobileCLIP-S2', 'mt5-base-ViT-B-32', 'mt5-xl-ViT-H-14', 'nllb-clip-base', 'nllb-clip-base-siglip', 'nllb-clip-large', 'nllb-clip-large-siglip', 'PE-Core-B-16', 'PE-Core-bigG-14-448', 'PE-Core-L-14-336', 'PE-Core-S-16-384', 'PE-Core-T-16-384', 'RN50', 'RN50-quickgelu', 'RN50x4', 'RN50x4-quickgelu', 'RN50x16', 'RN50x16-quickgelu', 'RN50x64', 'RN50x64-quickgelu', 'RN101', 'RN101-quickgelu', 'roberta-ViT-B-32', 'swin_base_patch4_window7_224', 'ViT-B-16', 'ViT-B-16-plus', 'ViT-B-16-plus-240', 'ViT-B-16-quickgelu', 'ViT-B-16-SigLIP', 'ViT-B-16-SigLIP2', 'ViT-B-16-SigLIP2-256', 'ViT-B-16-SigLIP2-384', 'ViT-B-16-SigLIP2-512', 'ViT-B-16-SigLIP-256', 'ViT-B-16-SigLIP-384', 'ViT-B-16-SigLIP-512', 'ViT-B-16-SigLIP-i18n-256', 'ViT-B-32', 'ViT-B-32-256', 'ViT-B-32-plus-256', 'ViT-B-32-quickgelu', 'ViT-B-32-SigLIP2-256', 'ViT-bigG-14', 'ViT-bigG-14-CLIPA', 'ViT-bigG-14-CLIPA-336', 'ViT-bigG-14-quickgelu', 'ViT-bigG-14-worldwide', 'ViT-bigG-14-worldwide-378', 'ViT-e-14', 'ViT-g-14', 'ViT-gopt-16-SigLIP2-256', 'ViT-gopt-16-SigLIP2-384', 'ViT-H-14', 'ViT-H-14-378', 'ViT-H-14-378-quickgelu', 'ViT-H-14-CLIPA', 'ViT-H-14-CLIPA-336', 'ViT-H-14-quickgelu', 'ViT-H-14-worldwide', 'ViT-H-14-worldwide-378', 'ViT-H-14-worldwide-quickgelu', 'ViT-H-16', 'ViT-L-14', 'ViT-L-14-280', 'ViT-L-14-336', 'ViT-L-14-336-quickgelu', 'ViT-L-14-CLIPA', 'ViT-L-14-CLIPA-336', 'ViT-L-14-quickgelu', 'ViT-L-14-worldwide', 'ViT-L-14-worldwide-quickgelu', 'ViT-L-16', 'ViT-L-16-320', 'ViT-L-16-SigLIP2-256', 'ViT-L-16-SigLIP2-384', 'ViT-L-16-SigLIP2-512', 'ViT-L-16-SigLIP-256', 'ViT-L-16-SigLIP-384', 'ViT-M-16', 'ViT-M-16-alt', 'ViT-M-32', 'ViT-M-32-alt', 'ViT-S-16', 'ViT-S-16-alt', 'ViT-S-32', 'ViT-S-32-alt', 'ViT-SO400M-14-SigLIP', 'ViT-SO400M-14-SigLIP2', 'ViT-SO400M-14-SigLIP2-378', 'ViT-SO400M-14-SigLIP-378', 'ViT-SO400M-14-SigLIP-384', 'ViT-SO400M-16-SigLIP2-256', 'ViT-SO400M-16-SigLIP2-384', 'ViT-SO400M-16-SigLIP2-512', 'ViT-SO400M-16-SigLIP-i18n-256', 'vit_medium_patch16_gap_256', 'vit_relpos_medium_patch16_cls_224', 'ViTamin-B', 'ViTamin-B-LTT', 'ViTamin-L', 'ViTamin-L2', 'ViTamin-L2-256', 'ViTamin-L2-336', 'ViTamin-L2-384', 'ViTamin-L-256', 'ViTamin-L-336', 'ViTamin-L-384', 'ViTamin-S', 'ViTamin-S-LTT', 'ViTamin-XL-256', 'ViTamin-XL-336', 'ViTamin-XL-384', 'xlm-roberta-base-ViT-B-32', 'xlm-roberta-large-ViT-H-14']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import open_clip\n",
        "\n",
        "model_name = \"ViT-B-32\"\n",
        "print(f\"\\nLoading CLIP Model: {model_name}\")\n",
        "\n",
        "model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained='openai')\n",
        "tokenizer = open_clip.get_tokenizer(model_name)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"\\nModel Loaded Successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283,
          "referenced_widgets": [
            "72a8b6e2789a4d86a7d46b213c87b9e0",
            "b996976df8024b6ea95f5dba1dd101a8",
            "4060bc0609c94c53a24a0d6654f1862d",
            "b8ad8a3a27fd4dbe96790c920c3339fc",
            "86c1262f5b844c13971320054d02f488",
            "5a1458a9a2ab452bac32443f0491b34d",
            "6363c8f072e746b2ad2f3f13192c6725",
            "de20ade2bc764f1b8c2d7203941c4ac3",
            "598434db3b1b461ea9706956fe35faa8",
            "baa031b9b54e4d50ad015445680f9b03",
            "4e593131c2bb4087bbe1bdf869d0810c"
          ]
        },
        "id": "l0e5nOthq0oM",
        "outputId": "f9f59f90-bfa4-428e-be47-b1e3c2277142",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading CLIP Model: ViT-B-32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72a8b6e2789a4d86a7d46b213c87b9e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Loaded Successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, csv_path, preprocess):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        self.images = df['image_path'].tolist()\n",
        "        self.texts = df['text'].astype(str).tolist()\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
        "        img = self.preprocess(img)\n",
        "        txt = self.texts[idx]\n",
        "        return img, txt"
      ],
      "metadata": {
        "id": "kID08XnbrUsD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CocoCaptions\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"awsaf49/coco-2017-dataset\")\n",
        "print(f\"\\nPath to Dataset Files: {path}\")\n",
        "\n",
        "coco2017_path = os.path.join(path, \"coco2017\")\n",
        "coco_root = os.path.join(coco2017_path, \"train2017\")\n",
        "ann_file = os.path.join(coco2017_path, \"annotations/captions_train2017.json\")\n",
        "train_csv_path = \"dataset_train.csv\"\n",
        "\n",
        "print()\n",
        "coco = CocoCaptions(root=coco_root, annFile=ann_file)\n",
        "print()\n",
        "\n",
        "if not os.path.exists(train_csv_path):\n",
        "    image_paths = []\n",
        "    texts = []\n",
        "\n",
        "    for idx in tqdm(range(len(coco)), desc=\"Processing COCO Captions\"):\n",
        "        img, captions = coco[idx]\n",
        "        img_path = os.path.join(coco_root, coco.coco.imgs[coco.ids[idx]]['file_name'])\n",
        "        for caption in captions:\n",
        "            image_paths.append(img_path)\n",
        "            texts.append(caption)\n",
        "\n",
        "    df = pd.DataFrame({\"image_path\": image_paths, \"text\": texts})\n",
        "    df.to_csv(train_csv_path, index=False)\n",
        "    print(f\"\\nSaved CSV To: {train_csv_path}\")\n",
        "else:\n",
        "    print(f\"\\nCSV File Already Exists: {train_csv_path}\")\n",
        "    df = pd.read_csv(train_csv_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "focrNyvcrn0t",
        "outputId": "ef43e1c4-ffeb-4cbe-e6d0-6477adcec658"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'coco-2017-dataset' dataset.\n",
            "\n",
            "Path to Dataset Files: /kaggle/input/coco-2017-dataset\n",
            "\n",
            "loading annotations into memory...\n",
            "Done (t=1.90s)\n",
            "creating index...\n",
            "index created!\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing COCO Captions: 100%|██████████| 118287/118287 [22:06<00:00, 89.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved CSV To: dataset_train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.groupby('image_path').apply(lambda x: x.sample(1)).reset_index(drop=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHvSmoZRetCh",
        "outputId": "f77d4788-5238-4bf1-b17b-9f866f823629"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-766484715.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df = df.groupby('image_path').apply(lambda x: x.sample(1)).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['image_id'] = df['image_path'].apply(lambda x: int(x.split('/')[-1].split('.')[0]))\n"
      ],
      "metadata": {
        "id": "3S77NW5bqStU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_subset = df.sample(n=5000, random_state=42).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "tLuJhFAsf5OR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ICT-Q\n"
      ],
      "metadata": {
        "id": "_7tqWhrU5qqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_subset_ictq = df_subset.drop(columns=[\"neg_img_idx\"])\n",
        "df_subset_ictq = df_subset"
      ],
      "metadata": {
        "id": "2QdHh40T5Pqr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers scikit-learn faiss-cpu pandas pillow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlyDudoc54E0",
        "outputId": "7c5823e3-dc07-42cc-cba3-9abf5c47e59d",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# ----------------------------\n",
        "# USER CONFIG\n",
        "# ----------------------------\n",
        "df = df_subset_ictq.reset_index(drop=True)\n",
        "assert \"image_path\" in df.columns and \"text\" in df.columns\n",
        "\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "EPOCHS = 7\n",
        "BATCH_SIZE = 32  # Increased for better GPU utilization\n",
        "LR = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "CLUSTER_EVERY_N_EPOCHS = 1  # Reduced frequency - clusters don't change much each epoch\n",
        "N_CLUSTERS = 40\n",
        "N_HARD_NEG_PER_QUERY = 7\n",
        "SEED = 67\n",
        "SAVE_DIR = \"./clip_ictq_model\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# ----------------------------\n",
        "# PRE-CACHE DATA\n",
        "# ----------------------------\n",
        "all_texts_list = df[\"text\"].astype(str).tolist()  # Cache as list for fast lookup\n",
        "\n",
        "# ----------------------------\n",
        "# MODEL + PROCESSOR\n",
        "# ----------------------------\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "model.train()\n",
        "\n",
        "# ----------------------------\n",
        "# DATASET\n",
        "# ----------------------------\n",
        "class CocoSubsetDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        return {\n",
        "            \"image_path\": row[\"image_path\"],\n",
        "            \"text\": str(row[\"text\"]),\n",
        "            \"idx\": int(idx),\n",
        "        }\n",
        "\n",
        "dataset = CocoSubsetDataset(df)\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=2  # Prefetch batches\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# HELPERS\n",
        "# ----------------------------\n",
        "@torch.no_grad()\n",
        "def compute_text_embeddings(model, processor, texts, batch_size=128, device=DEVICE):\n",
        "    model.eval()\n",
        "    embs = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing embeddings\"):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        text_feats = model.get_text_features(**inputs)\n",
        "        text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
        "        embs.append(text_feats.cpu())\n",
        "    embs = torch.cat(embs, dim=0)\n",
        "    model.train()\n",
        "    return embs.numpy()\n",
        "\n",
        "def build_clusters(embeddings, n_clusters=N_CLUSTERS):\n",
        "    kmeans = MiniBatchKMeans(\n",
        "        n_clusters=min(n_clusters, max(2, embeddings.shape[0]//5)),\n",
        "        random_state=SEED,\n",
        "        batch_size=2000,  # Increased batch size\n",
        "        n_init=3\n",
        "    )\n",
        "    cluster_ids = kmeans.fit_predict(embeddings)\n",
        "    return cluster_ids\n",
        "\n",
        "def build_cluster_map(cluster_ids):\n",
        "    cluster_map = defaultdict(list)\n",
        "    for i, c in enumerate(cluster_ids):\n",
        "        cluster_map[int(c)].append(i)\n",
        "    return cluster_map\n",
        "\n",
        "# ----------------------------\n",
        "# INITIAL CLUSTERING\n",
        "# ----------------------------\n",
        "print(\"Computing initial text embeddings...\")\n",
        "text_embs = compute_text_embeddings(model, processor, all_texts_list, batch_size=128)\n",
        "cluster_ids = build_clusters(text_embs, n_clusters=N_CLUSTERS)\n",
        "cluster_map = build_cluster_map(cluster_ids)\n",
        "print(f\"Created {len(cluster_map)} clusters\")\n",
        "\n",
        "# ----------------------------\n",
        "# TRAINING\n",
        "# ----------------------------\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "temperature = 1.0\n",
        "\n",
        "global_step = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0.0\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for batch in pbar:\n",
        "        image_paths = batch[\"image_path\"]\n",
        "        texts = batch[\"text\"]\n",
        "        idxs = batch[\"idx\"].numpy()\n",
        "        B = len(texts)\n",
        "\n",
        "        # Collect hard negative indices\n",
        "        hard_neg_indices = []\n",
        "        for i, global_idx in enumerate(idxs):\n",
        "            c = int(cluster_ids[global_idx])\n",
        "            candidates = [x for x in cluster_map[c] if x != global_idx]\n",
        "\n",
        "            if len(candidates) >= N_HARD_NEG_PER_QUERY:\n",
        "                sampled = random.sample(candidates, N_HARD_NEG_PER_QUERY)\n",
        "            else:\n",
        "                pool = [x for x in range(len(df)) if x != global_idx]\n",
        "                sampled = random.sample(pool, N_HARD_NEG_PER_QUERY)\n",
        "            hard_neg_indices.extend(sampled)\n",
        "\n",
        "        # Build text pool - use cached list instead of df.loc\n",
        "        text_pool_texts = list(texts)  # positives\n",
        "        text_pool_texts.extend([all_texts_list[i] for i in hard_neg_indices])  # hard negatives\n",
        "\n",
        "        # Load images\n",
        "        imgs = [Image.open(p).convert(\"RGB\") for p in image_paths]\n",
        "\n",
        "        # Process inputs\n",
        "        inputs = processor(\n",
        "            text=text_pool_texts,\n",
        "            images=imgs,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        text_features = outputs.text_embeds\n",
        "        image_features = outputs.image_embeds\n",
        "\n",
        "        # Normalize\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Compute logits\n",
        "        logits = image_features @ text_features.t() / temperature\n",
        "        labels = torch.arange(B, device=DEVICE)\n",
        "\n",
        "        # Bidirectional loss\n",
        "        loss_img_to_text = criterion(logits, labels)\n",
        "        logits_t2i = logits.t()\n",
        "        logits_pos_texts = logits_t2i[:B, :]\n",
        "        loss_text_to_img = criterion(logits_pos_texts, labels)\n",
        "        loss = (loss_img_to_text + loss_text_to_img) / 2.0\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        global_step += 1\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1} finished. Avg loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Re-cluster periodically\n",
        "    if (epoch + 1) % CLUSTER_EVERY_N_EPOCHS == 0 and (epoch + 1) < EPOCHS:\n",
        "        print(f\"Recomputing text embeddings & reclustering...\")\n",
        "        text_embs = compute_text_embeddings(model, processor, all_texts_list, batch_size=128)\n",
        "        cluster_ids = build_clusters(text_embs, n_clusters=N_CLUSTERS)\n",
        "        cluster_map = build_cluster_map(cluster_ids)\n",
        "        print(f\"Re-clustering done. {len(cluster_map)} clusters\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    ckpt_path = os.path.join(SAVE_DIR, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
        "    torch.save(model.state_dict(), ckpt_path)\n",
        "    print(f\"Saved checkpoint to {ckpt_path}\")\n",
        "\n",
        "# Final save\n",
        "model.save_pretrained(SAVE_DIR)\n",
        "processor.save_pretrained(SAVE_DIR)\n",
        "print(\"Training complete. Model saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz5YN_51-YAU",
        "outputId": "a6f02bfd-3c3e-4c51-8d18-e40b66873bd9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing initial text embeddings...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|██████████| 40/40 [00:03<00:00, 11.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 40 clusters\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/7:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 1/7: 100%|██████████| 157/157 [03:01<00:00,  1.15s/it, loss=2.5672]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 finished. Avg loss: 3.8044\n",
            "Recomputing text embeddings & reclustering...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing embeddings: 100%|██████████| 40/40 [00:03<00:00, 10.86it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Re-clustering done. 40 clusters\n",
            "Saved checkpoint to ./clip_ictq_model/checkpoint_epoch_1.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/7:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 2/7: 100%|██████████| 157/157 [03:03<00:00,  1.17s/it, loss=2.3328]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 finished. Avg loss: 3.6721\n",
            "Recomputing text embeddings & reclustering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing embeddings: 100%|██████████| 40/40 [00:03<00:00, 10.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-clustering done. 40 clusters\n",
            "Saved checkpoint to ./clip_ictq_model/checkpoint_epoch_2.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 3/7:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 3/7: 100%|██████████| 157/157 [03:02<00:00,  1.16s/it, loss=2.3476]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 finished. Avg loss: 3.6389\n",
            "Recomputing text embeddings & reclustering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing embeddings: 100%|██████████| 40/40 [00:03<00:00, 10.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-clustering done. 40 clusters\n",
            "Saved checkpoint to ./clip_ictq_model/checkpoint_epoch_3.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 4/7:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 4/7: 100%|██████████| 157/157 [03:04<00:00,  1.17s/it, loss=2.3567]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 finished. Avg loss: 3.6278\n",
            "Recomputing text embeddings & reclustering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing embeddings: 100%|██████████| 40/40 [00:03<00:00, 11.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-clustering done. 40 clusters\n",
            "Saved checkpoint to ./clip_ictq_model/checkpoint_epoch_4.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 5/7:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 5/7: 100%|██████████| 157/157 [03:05<00:00,  1.18s/it, loss=2.3106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 finished. Avg loss: 3.6326\n",
            "Recomputing text embeddings & reclustering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing embeddings: 100%|██████████| 40/40 [00:03<00:00, 10.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-clustering done. 40 clusters\n",
            "Saved checkpoint to ./clip_ictq_model/checkpoint_epoch_5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 6/7:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 6/7: 100%|██████████| 157/157 [03:05<00:00,  1.18s/it, loss=2.3897]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 finished. Avg loss: 3.6172\n",
            "Recomputing text embeddings & reclustering...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing embeddings: 100%|██████████| 40/40 [00:03<00:00, 10.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-clustering done. 40 clusters\n",
            "Saved checkpoint to ./clip_ictq_model/checkpoint_epoch_6.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 7/7:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Epoch 7/7: 100%|██████████| 157/157 [03:04<00:00,  1.17s/it, loss=2.3417]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 finished. Avg loss: 3.6050\n",
            "Saved checkpoint to ./clip_ictq_model/checkpoint_epoch_7.pt\n",
            "Training complete. Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import CocoCaptions\n",
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"awsaf49/coco-2017-dataset\")\n",
        "print(f\"\\nPath to Dataset Files: {path}\")\n",
        "\n",
        "coco2017_path = os.path.join(path, \"coco2017\")\n",
        "coco_root = os.path.join(coco2017_path, \"train2017\")\n",
        "ann_file = os.path.join(coco2017_path, \"annotations/captions_train2017.json\")\n",
        "train_csv_path = \"dataset_train.csv\"\n",
        "\n",
        "print()\n",
        "coco = CocoCaptions(root=coco_root, annFile=ann_file)\n",
        "print()\n",
        "\n",
        "if not os.path.exists(train_csv_path):\n",
        "    image_paths = []\n",
        "    texts = []\n",
        "\n",
        "    for idx in tqdm(range(len(coco)), desc=\"Processing COCO Captions\"):\n",
        "        img, captions = coco[idx]\n",
        "        img_path = os.path.join(coco_root, coco.coco.imgs[coco.ids[idx]]['file_name'])\n",
        "        for caption in captions:\n",
        "            image_paths.append(img_path)\n",
        "            texts.append(caption)\n",
        "\n",
        "    df_og = pd.DataFrame({\"image_path\": image_paths, \"text\": texts})\n",
        "    df_og.to_csv(train_csv_path, index=False)\n",
        "    print(f\"\\nSaved CSV To: {train_csv_path}\")\n",
        "else:\n",
        "    print(f\"\\nCSV File Already Exists: {train_csv_path}\")\n",
        "    df_og = pd.read_csv(train_csv_path)"
      ],
      "metadata": {
        "id": "PMEuUHwBDHwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e12fb3d-9c9f-47bb-aeec-de9122ea98d7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'coco-2017-dataset' dataset.\n",
            "\n",
            "Path to Dataset Files: /kaggle/input/coco-2017-dataset\n",
            "\n",
            "loading annotations into memory...\n",
            "Done (t=0.91s)\n",
            "creating index...\n",
            "index created!\n",
            "\n",
            "\n",
            "CSV File Already Exists: dataset_train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_og = df_og.groupby('image_path').apply(lambda x: x.sample(1)).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "coS7zm9MD6Yr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171f78a6-c0c6-40c8-b300-ddb8f433dc8a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-763703059.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  df_og = df_og.groupby('image_path').apply(lambda x: x.sample(1)).reset_index(drop=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Remove the train subset from the full dataframe\n",
        "df_remaining = df_og.drop(df_subset.index)\n",
        "\n",
        "# Step 2: Sample exactly 1,000 rows for test set\n",
        "df_test = df_remaining.sample(n=1000, random_state=42)\n",
        "\n",
        "# Optional: Reset indices\n",
        "df_test = df_test.reset_index(drop=True)\n",
        "df_remaining = df_remaining.reset_index(drop=True)\n",
        "\n",
        "print(\"Train size:\", len(df_subset))\n",
        "print(\"Test size:\", len(df_test))\n",
        "print(\"Remaining (unused):\", len(df_remaining))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QJ5GQINzcDs",
        "outputId": "a244a3f1-22a2-4c3d-bafa-f4254ed7c3eb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 5000\n",
            "Test size: 1000\n",
            "Remaining (unused): 113287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ----------------------------\n",
        "# Config\n",
        "# ----------------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Load your trained model\n",
        "model_path = \"./clip_ictq_model\"\n",
        "model = CLIPModel.from_pretrained(model_path).to(DEVICE)\n",
        "processor = CLIPProcessor.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "# ----------------------------\n",
        "# Dataset for testing\n",
        "# ----------------------------\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        return {\n",
        "            \"image_path\": row[\"image_path\"],\n",
        "            \"text\": str(row[\"text\"]),\n",
        "            \"idx\": int(idx)\n",
        "        }\n",
        "\n",
        "test_dataset = TestDataset(df_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ----------------------------\n",
        "# Compute embeddings\n",
        "# ----------------------------\n",
        "all_image_embeds = []\n",
        "all_text_embeds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Computing embeddings\"):\n",
        "        # Images\n",
        "        images = [Image.open(p).convert(\"RGB\") for p in batch[\"image_path\"]]\n",
        "        texts = batch[\"text\"]\n",
        "\n",
        "        inputs = processor(\n",
        "            text=texts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Normalize embeddings\n",
        "        img_embeds = outputs.image_embeds / outputs.image_embeds.norm(dim=-1, keepdim=True)\n",
        "        txt_embeds = outputs.text_embeds / outputs.text_embeds.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        all_image_embeds.append(img_embeds.cpu())\n",
        "        all_text_embeds.append(txt_embeds.cpu())\n",
        "\n",
        "# Concatenate embeddings\n",
        "all_image_embeds = torch.cat(all_image_embeds, dim=0)  # shape: (num_images, dim)\n",
        "all_text_embeds = torch.cat(all_text_embeds, dim=0)    # shape: (num_texts, dim)\n",
        "\n",
        "# ----------------------------\n",
        "# Compute similarity: Text -> Image\n",
        "# ----------------------------\n",
        "similarity = all_text_embeds @ all_image_embeds.T  # (num_texts, num_images)\n",
        "labels = torch.arange(len(df_test))  # ground-truth indices\n",
        "\n",
        "# ----------------------------\n",
        "# Compute Text->Image retrieval metrics\n",
        "# ----------------------------\n",
        "def compute_retrieval_metrics(similarity, labels):\n",
        "    num_queries = similarity.size(0)\n",
        "    ranks = []\n",
        "    rr = []\n",
        "\n",
        "    for i in range(num_queries):\n",
        "        sim_row = similarity[i]  # similarity scores for text i\n",
        "        sorted_indices = torch.argsort(sim_row, descending=True)\n",
        "        rank = (sorted_indices == labels[i]).nonzero(as_tuple=True)[0].item()\n",
        "        ranks.append(rank + 1)\n",
        "        rr.append(1.0 / (rank + 1))\n",
        "\n",
        "    ranks = np.array(ranks)\n",
        "    rr = np.array(rr)\n",
        "\n",
        "    r1 = np.mean(ranks <= 1)\n",
        "    r5 = np.mean(ranks <= 5)\n",
        "    r10 = np.mean(ranks <= 10)\n",
        "    medr = np.median(ranks)\n",
        "    mrr = np.mean(rr)\n",
        "\n",
        "    return {\"R@1\": r1, \"R@5\": r5, \"R@10\": r10, \"MedR\": medr, \"MRR\": mrr}\n",
        "\n",
        "metrics_t2i = compute_retrieval_metrics(similarity, labels)\n",
        "\n",
        "print(\"Text -> Image Retrieval Metrics:\")\n",
        "for k, v in metrics_t2i.items():\n",
        "    if k.startswith(\"R@\"):\n",
        "        print(f\"{k}: {v*100:.2f}%\")\n",
        "    else:\n",
        "        print(f\"{k}: {v:.2f}\")\n"
      ],
      "metadata": {
        "id": "6tF909LxHAIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fe7af3e-e0e4-4a87-e314-f4ee1e079382"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing embeddings: 100%|██████████| 32/32 [00:13<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text -> Image Retrieval Metrics:\n",
            "R@1: 14.00%\n",
            "R@5: 37.60%\n",
            "R@10: 54.10%\n",
            "MedR: 9.00\n",
            "MRR: 0.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tWU7BaXVHSOw"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}